{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <h1 style=\"color:blue;text-align:left\">Inteligencia Artificial</h1></td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Notebook</p></tp>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Arquitecturas de agentes</p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "En este notebook ejemplificaremos los dos tipos más sencillos de agente: dirigido por tabla y de respuesta simple. Usaremos el problema del laberinto para implementar los programa de agente para intentar salir del laberinto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secciones\n",
    "\n",
    "Desarrollaremos la explicación en las siguientes secciones:\n",
    "\n",
    "1. [El ambiente del laberinto](#lab)\n",
    "2. [Un agente dirigido por tabla](#agenteTD)\n",
    "3. [Un agente de reflejo simple](#agenteSR)\n",
    "4. [Pruebas de desempeño](#pruebas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El ambiente del laberinto <a class=\"anchor\" id=\"lab\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "En el problema del laberinto la percepción del agente está basada en una colección de sensores que se encienden cuando detectan un obstáculo, organizados de la siguiente manera:\n",
    "\n",
    "`[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]`\n",
    "\n",
    "Los valores de los sensores se obtienen mediante el método `para_sentidos()` de la clase `Laberinto` y se almacenan en el atributo `perceptos` de un objeto de clase `Agente`. Veamos la implementación de la percepción del agente directamente en el ejemplo donde el agente comienza en la casilla $(11,11)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La lista de sensores es:\n",
      "[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]\n",
      "Los valores en la casilla (11,11) son:\n",
      "[False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "from agentes import *\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "lab.pintar()\n",
    "agente = Agente()\n",
    "agente.perceptos = lab.para_sentidos()\n",
    "print('La lista de sensores es:')\n",
    "print('[sensor frontal, sensor izquierdo, sensor derecho, sensor trasero]')\n",
    "print('Los valores en la casilla (11,11) son:')\n",
    "print(agente.perceptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las acciones posibles del agente son:\n",
    "\n",
    "* adelante: el agente avanza una casilla.\n",
    "* voltearIzquierda: el agente gira 90º en contra de las manecillas del reloj.\n",
    "* voltearDerecha: el agente gira 90º a favor de las manecillas del reloj.\n",
    "\n",
    "Cada acción del agente tiene un efecto en el entorno, implementado mediante el método `transicion()` de la clase `Laberinto`. Veamos un ejemplo en donde el agente parte de la casilla $(11,11)$ y deambula un poco por el laberinto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentes import *\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "acciones = ['adelante', 'adelante', 'voltearIzquierda', \n",
    "            'adelante', 'adelante', 'adelante', 'voltearDerecha', 'adelante']\n",
    "\n",
    "lab.pintar()\n",
    "\n",
    "for a in acciones:\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:**\n",
    "\n",
    "Deambule aleatoriamente por diez pasos y presente en cada una de ellas lo que perciben los sensores del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente dirigido por tabla <a class=\"anchor\" id=\"agenteTD\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El tipo más sencillo de un programa de agente es cuando hacemos una asociación directa entre input y output, en este caso, entre percepción y acción. Esta asociación se puede realizar mediante una tabla. \n",
    "\n",
    "Definimos la siguiente tabla (que hasta ahora sólo está definida parcialmente), la cual vincula perceptos con acciones. La tabla implementa la idea de que si el agente percibe que el frente no está bloqueado y el flanco derecho está bloqueado, entonces avanza una casilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (sensor frontal, sensor izquierdo, sensor derecho, sensor trasero)\n",
    "tabla = {\n",
    "    # Si el frente no está bloqueado y el flanco derecho está\n",
    "    # bloqueado, avanzar una casilla\n",
    "    (False, True, True, True):['adelante'],\n",
    "    (False, True, True, False):['adelante'],\n",
    "    (False, False, True, True):['adelante'],\n",
    "    (False, False, True, False):['adelante'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que esta regla tan simple requiere ser expresada mediante cuatro filas de la tabla.\n",
    "\n",
    "\n",
    "Ahora incluimos el programa dirigido por tabla como el método `programa()` de la clase `Agente`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programaTD(self):\n",
    "    self.acciones += self.tabla[tuple(self.perceptos)]\n",
    "\n",
    "setattr(Agente, 'programa', programaTD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observemos cómo trabaja el agente que implementa este programa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "agente = Agente()\n",
    "agente.tabla = tabla\n",
    "\n",
    "for i in range(20):\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    a = agente.reaccionar() # <= ver definición en agentes.py\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error `KeyError: (True, False, True, False)` ocurre porque la tabla no tiene ninguna fila para el percepto `(True, False, True, False)` y entonces no puede determinar ninguna acción a tomar. ¡Observe que el agente no sabe qué acción tomar en ninguna situación cuando hay un muro enfrente! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2:**\n",
    "\n",
    "El agente llega hasta que se topa con un muro y no sabe qué hacer. Extienda la tabla anterior para incluir las líneas que determinan que \"si el frente y el flanco derecho están bloqueados y el flanco izquierdo no está bloqueado, voltear a la izquierda\".\n",
    "\n",
    "Visualice el funcionamiento del agente para comprobar su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3:**\n",
    "\n",
    "En la tabla falta incluir instrucciones que digan que \"si el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla\". Extienda la tabla del ejercicio 2 para incluir las líneas que implementan esta regla. Visualice el funcionamiento del agente comenzando desde la casilla $(11,11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4:**\n",
    "\n",
    "* ¿Qué pasa cuando el agente comienza en la casilla $(6,3)$? Describa el comportamiento del agente y explique si el programa de agente resulta adecuado o no para este caso.\n",
    "\n",
    "* ¿Qué pasa cuando el agente comienza en la casilla $(7,3)$? Describa el comportamiento del agente y explique si el programa de agente resulta adecuado o no para este caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un agente de reflejo simple <a class=\"anchor\" id=\"agenteSR\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "El proceso de escribir una tabla es bastante dispendioso, pues hay que considerar una gran cantidad de combinaciones de valores para los sensores. Un tipo de agente de nivel un poco más elevado que resuelve esta situación son los agentes de reflejo simple. El programa de este tipo de agentes está basado en relgas de condición-acción, las cuales relacionan condiciones sobre los sensores y las acciones. Al considerar condiciones en lugar de combinaciones de valores, la escritura es más eficiente.\n",
    "\n",
    "A continuación presentamos una posible implementación de un agente de reflejo simple para el problema del laberinto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptos[0]  =>  sensor forntal\n",
    "# perceptos[1]  =>  sensor izquierdo\n",
    "# perceptos[2]  =>  sensor derecho\n",
    "# perceptos[3]  =>  sensor trasero\n",
    "reglas = {\n",
    "    # Si el frente no está bloqueado y el flanco derecho está\n",
    "    # bloqueado, avanzar una casilla\n",
    "    'not self.perceptos[0] and self.perceptos[2]': ['adelante'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def programaSR(self):\n",
    "    reaccion = self.reglas\n",
    "    for antecedente in self.reglas:\n",
    "        if eval(antecedente):\n",
    "            self.acciones += reaccion[antecedente]\n",
    "            break\n",
    "\n",
    "setattr(Agente, 'programa', programaSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from agentes import *\n",
    "\n",
    "lab = Laberinto(pos_inicial=(11,11))\n",
    "agente = Agente()\n",
    "agente.reglas = reglas\n",
    "\n",
    "for i in range(50):\n",
    "    agente.perceptos = lab.para_sentidos()\n",
    "    a = agente.reaccionar()\n",
    "    lab.transicion(a)\n",
    "    clear_output(wait=True)\n",
    "    lab.pintar()\n",
    "    plt.show()\n",
    "    sleep(.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error `IndexError: pop from empty list` ocurre porque la lista de acciones es vacía, toda vez que el programa aún no está equipado para dar una decisión cuando hay un muro enfrente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5:**\n",
    "\n",
    "Extienda el programa de agente anterior para implementar las reglas condición-acción siguientes:\n",
    "\n",
    "* Si el frente y el flanco derecho están bloqueados pero el flanco izquierdo no está bloqueado, voltear a la izquierda.\n",
    "* Si el flanco derecho no está bloqueado, voltear a la derecha y avanzar una casilla.\n",
    "\n",
    "Visualice el funcionamiento del agente desde la casilla $(11,11)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6:**\n",
    "\n",
    "Complete las reglas del agente para que siempre tome alguna dirección y no aparezca el error `pop from empty list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 7:**\n",
    "\n",
    "¿Cree usted que es posible implementar un agente de respuesta simple para encontrar una salida al comenzar en las casillas $(6,3)$ y $(7,3)$? Justifique su respuesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas de desempeño <a class=\"anchor\" id=\"pruebas\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 8:**\n",
    "\n",
    "Implemente la función `lab_aleatorio(p)` para generar un laberinto de $12\\times 12$ de manera aleatoria, de tal manera que en cada casilla, excepto la salida $(0,0)$ y la inicial $(11,11)$, haya un muro con probabilidad `p` (con $0\\leq p\\leq 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 9:**\n",
    "\n",
    "Determine el porcentaje de éxito del agente de respuesta simple en 100 laberintos aleatorios con $p=0.2$, iniciando en la casilla $(11,11)$, y una cantidad máxima de pasos igual a 200.\n",
    "\n",
    "**Nota:**\n",
    "\n",
    "El porcentaje de éxito debe estar cerca del 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 10:**\n",
    "\n",
    "Dibuje el porcentaje de éxito como una función de $p$ en las mismas condiciones del ejercicio pasado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este notebook usted aprendió\n",
    "\n",
    "* Los detalles de la implementación de un programa de agente dirigido por tablas y de reflejo simple.\n",
    "* Realizar pruebas de desempeño a los programas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
