{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpmFfXsQ0dYI"
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"./imagenes/Macc.png\" width=\"400\"/></td>\n",
    "        <td>&nbsp;</td>\n",
    "        <td>\n",
    "            <h1 style=\"color:blue;text-align:left\">Inteligencia Artificial</h1></td>\n",
    "        <td>\n",
    "            <table><tr>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Notebook</p></tp>\n",
    "            <tp><p style=\"font-size:150%;text-align:center\">Juegos (2/3) </p></tp>\n",
    "            </tr></table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "## Objetivo <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "Hemos visto el problema de ganar un juego de suma cero contra un humano mediante el método Minimax. Ahora veremos cómo hacer más eficiente el tiempo de búsqueda al podar el árbol de conforntación mediante el algoritmo alfa-beta. También, veremos funciones de evaluación que nos permitirán interrumpir la búsqueda a una altura determinada del árboly así mejorar la eficiencia y mantener un buen desempeño del agente. \n",
    "\n",
    "Adaptado de (Russell & Norvig, 2020), sección 5.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "## Secciones\n",
    "\n",
    "Desarrollaremos la explicación de la siguiente manera:\n",
    "\n",
    "* [Algoritmo de poda alfa-beta](#alfa-beta).\n",
    "* [Funciones de evaluación](#feval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo de poda alfa-beta <a class=\"anchor\" id=\"alfa-beta\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Para explicar el funcionamiento de la poda alfa-beta consideremos un ejemplo más bien simple. Supongamos un juego que define el siguiente árbol, con la utilidad de los estados terminales ya puestos sobre la figura debajo de cada hoja, y los nombres de las acciones posibles etiquetando cada arista:\n",
    "\n",
    "<img src=\"imagenes/alfa-beta.png\" width=\"350px\">\n",
    "\n",
    "Este árbol lo hemos puesto para ilustrar el comportamiento del juego. No obstante, es muy importante observar que la idea del algoritmo es evitar tener que construir todo el árbol, **deteniendo la exploración del mismo cuando sabemos que una acción no es conveniente para el jugador respectivo**. Vamos a ver cómo se logra esto. \n",
    "\n",
    "La creación del árbol de confrontación se hace explorando los hijos de cada estado. Durante esta exploración **se llevará un registro del valor máximo de las acciones hasta ahora.** Este es el valor alfa. **También se llevará un registro del valor mínimo (beta).** Veamos en nuestro ejemplo para qué llevamos este registro de alfa (el funcionamiento de beta es análogo intercambiando jugadores). \n",
    "\n",
    "El algoritmo comienza a explorar el árbol primero en profundidad, inicializado con un valor de alfa de $-\\infty$ y de beta de $\\infty$. Estamos asumiendo que en la raíz (estado A) le corresponde el turno a MAX:\n",
    "\n",
    "<img src=\"imagenes/alfa-beta-1.png\" width=\"350px\">\n",
    "\n",
    "Una vez se obtiene el valor minimax para el primer hijo explorado (en este caso B), se puede asignar un valor a alfa (en este caso, 3):\n",
    "\n",
    "<img src=\"imagenes/alfa-beta-2.png\" width=\"350px\">\n",
    "\n",
    "Este valor de alfa se usa en la exploración del siguiente hijo, en este caso el estado C. Esto es, en la expansión de los hijos de C (subexpansiones) se utiliza el valor de alfa = 3. Aquí observamos que la primera subexpansión representa una utilidad de 2, que es inferior a alfa. Este es el criterio para detener la exploración de C.\n",
    "\n",
    "<img src=\"imagenes/alfa-beta-3.png\" width=\"350px\">\n",
    "\n",
    "La razón de esta detención debe ser clara. Para establecer el valor minimax del estado C, MIN buscará una opción con mínimo valor minimax. Este valor será menor o igual a 2. Esto permite deducir que en el estado A, MAX no tomará la acción que lo lleve al estado C, toda vez que el valor minimax de una acción ya explorada le reporta un mejor valor (a saber, la acción que lo lleva al estado B). Es por esto que no tiene sentido seguir explorando el estado C y toda esta rama puede podarse. \n",
    "\n",
    "### Pseudo código de la poda alfa-beta\n",
    "\n",
    "El siguiente es el pseudo código para el algoritmo de poda alfa-beta:\n",
    "\n",
    "<img src=\"imagenes/poda-alfa-beta.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1:**\n",
    "\n",
    "Implemente un código python `alpha_beta_search` con la poda alfa-beta y corra el algoritmo desde la raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from juegos import Triqui\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def alpha_beta_search(game: Triqui,\n",
    "                      state: np.matrix):\n",
    "    \n",
    "    player      = game.a_jugar(state)\n",
    "    value, move = max_value_alfa_beta(game  = game,\n",
    "                                      state = state,\n",
    "                                      alpha = -np.inf,\n",
    "                                      beta  = np.inf)\n",
    "    \n",
    "    return move \n",
    "\n",
    "def max_value_alfa_beta(game:  Triqui,\n",
    "                        state: np.matrix,\n",
    "                        alpha:   float,\n",
    "                        beta:   float):\n",
    "    \n",
    "    pass\n",
    "\n",
    "def min_value_alfa_beta(game:  Triqui,\n",
    "                        state: np.matrix,\n",
    "                        alpha:   float,\n",
    "                        beta:   float):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2:**\n",
    "\n",
    "Adapte la función `alpha_beta_search` para que la decisión que se tome sea sensible al jugador al que le corresponde el turno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de tiempos\n",
    "\n",
    "Vamos a revisar qué tanto más es eficiente el algoritmo de poda alfa-beta respecto al algoritmo original minimax, para el caso que nos ocupa de implementar el triqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiempos import compara_funciones\n",
    "from copy import deepcopy\n",
    "from minimax import minimax_search\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "funs = [\n",
    "    lambda s: minimax_search(tri, s),\n",
    "    lambda s: alpha_beta_search(tri, s),\n",
    "]\n",
    "nombres = [\n",
    "    'Minmax', \n",
    "    'Alfa-Beta', \n",
    "]\n",
    "tri = Triqui()\n",
    "s = tri.estado_inicial\n",
    "jugadas = [(0, 0)]#, (1, 1), (0, 1), (0, 2), (2, 0)]#, (1, 0), (1, 2), (2, 1), (2, 2)]\n",
    "lista_dfs = []\n",
    "for i, a in enumerate(jugadas):\n",
    "    data = compara_funciones(funs=funs, arg=s, nombres=nombres, N=1)\n",
    "    data['No. de jugada'] = i\n",
    "    lista_dfs.append(data)\n",
    "    s = deepcopy(tri.resultado(s, a))\n",
    "data = pd.concat(lista_dfs)\n",
    "# Graficando\n",
    "fig, ax = plt.subplots(1,1, figsize=(3*len(funs),3), tight_layout=True)\n",
    "sns.lineplot(data=data, hue='Función', x='No. de jugada', y='Tiempo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que, en este caso, la poda alfa-beta hace que la búsqueda de una decisión sea muy eficiente, comparada con la búsqueda del algoritmo minimax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine con poda alfa-beta\n",
    "\n",
    "Podemos jugar de nuevo el triqui, esta vez con menos demora en la toma de decisiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corra esta celda para comenzar un juego nuevo\n",
    "\n",
    "from ambientes import Triqui\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tri = Triqui()\n",
    "s = tri.estado_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzca aquí su jugada, corra la celda y espere\n",
    "# la jugada de O. Luego, vuelva a cambiar aquí mismo \n",
    "# su jugada y corra la celda de nuevo, etc.\n",
    "a = (1,1)\n",
    "\n",
    "assert(a in tri.acciones(s)), 'Acción no permitida. Intente de nuevo.'\n",
    "\n",
    "###################\n",
    "s = tri.resultado(s, a)\n",
    "#clear_output(wait=True)\n",
    "tri.pintar_estado(s)\n",
    "plt.show()\n",
    "\n",
    "# Computador responde\n",
    "if not tri.es_terminal(s):\n",
    "    a = alpha_beta_search(tri, s)\n",
    "    s = tri.resultado(s, a)\n",
    "    sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    tri.pintar_estado(s)\n",
    "    plt.show()\n",
    "    if tri.es_terminal(s):\n",
    "        print('Juego terminado. ¡Gana O!')\n",
    "else:\n",
    "    jugador = tri.a_jugar(s)\n",
    "    if tri.utilidad(s, jugador)==0:\n",
    "        print('Juego terminado. ¡Empate!')\n",
    "    else:\n",
    "        print('Juego terminado. ¡Gana X!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de evaluación <a class=\"anchor\" id=\"feval\"></a>\n",
    "\n",
    "([Volver al inicio](#inicio))\n",
    "\n",
    "Vamos a ver ahora un ejemplo muy bonito en el que se requiere usar funciones de evaluación para darle un valor a un estado. Observe que los dos algoritmos anteriores atribuyen valor minimax a los estados a partir de la utilidad de los estados terminales. Estos algoritmos toman esta utilidad y la 'suben' por los estados hasta la raíz. Pero, ¿qué pasa si es muy ineficiente llegar hasta el estado final para obtener una utilidad? Esto ocurre en el juego del ajedrez, en donde la explosión de estados es exponencial. Por ejemplo, expandir un tablero tres jugadas hacia adelante implica considerar alrededor de 726 millones de estados.\n",
    "\n",
    "Para evitar tener que bajar hasta los estados terminales, se puede usar una función `is_cutoff(s)`, la cual utiliza un criterio de detención para la expansión de estados. Esta función reemplazará el criterio `es_terminal` en los algoritmos minimax y poda alfa-beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cutoff(self, board, d):\n",
    "    if self.es_terminal(board):\n",
    "        return True\n",
    "    elif d >= max_lim:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se 'dispara' la condición `is_cutoff` y el estado no es terminal, debemos retornar un valor. Este valor estará dado por una función de evaluación, que debe aproximar qué tan bueno es un estado. Si el estado es mejor para MAX, la función de evaluación debe retornar valores positivos grandes; si es mejor para MIN, debe retornar valores negativos grandes. Veamos esto en el ejemplo del ajedrez.\n",
    "\n",
    "### Torre y rey contra rey solitario\n",
    "\n",
    "Uno de los finales que todo principiante en ajedrez debe aprender es el de hacer mate mediante torre y rey contra rey solitario. Una implementación en python usando la librería [python-chess](https://python-chess.readthedocs.io/en/latest/) es la siguiente. Note que las acciones usan la [notación algebráica estándar](https://es.wikipedia.org/wiki/Notaci%C3%B3n_algebraica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambientes import ReyTorreRey\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "roo = ReyTorreRey(tablero_inicial=2)\n",
    "s = roo.estado_inicial\n",
    "s\n",
    "\n",
    "# Juegan las negras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este tablero se puede hacer mate en dos jugadas. El mate se obtiene después de:\n",
    "\n",
    "1.  ...  Ka3\n",
    "2. Rc4  Ka2\n",
    "3. Ra4++\n",
    "\n",
    "Al introducir cada jugada, usaremos el método `jugada_manual`, en lugar de `resultado`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = roo.jugada_manual(s, 'Ka3')\n",
    "print('Mate?', roo.es_terminal(s))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = roo.jugada_manual(s, 'Rc4')\n",
    "print('Mate?', roo.es_terminal(s))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = roo.jugada_manual(s, 'Ka2')\n",
    "print('Mate?', roo.es_terminal(s))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = roo.jugada_manual(s, 'Ra4')\n",
    "print('Mate?', roo.es_terminal(s))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que, a pesar de que el mate está tan cerca, la creación de un árbol de confrontación mediante la metodología depth-first se quedará atascada en estados en los cuales el rey blanco se aleja progresivamente del rey negro. Para evitar este descenso, usamos el criterio `is_cutoff`, junto con una función de evaluación.\n",
    "\n",
    "La función de evaluación que proponemos es darle una mejor evaluación a las blancas (MAX) cuanto más arrinconado esté el rey negro. El siguiente tablero muestra el valor de la función de evaluación dependiendo de la casilla en que se encuentre el rey negro:\n",
    "\n",
    "<img src=\"imagenes/rincon.png\" width=\"350px\">\n",
    "\n",
    "Por ejemplo, si el rey negro está en e3, la función de evaluación retornará -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(self, board, jugador):\n",
    "    if self.es_terminal(board):\n",
    "        return self.utilidad(board, jugador)\n",
    "    else:\n",
    "        # Contamos rey negro en borde\n",
    "        fila, columna = self.casilla_pieza(board, 'k')\n",
    "        rey_negro_fila = (4 - fila if fila < 4 else (fila % 4) + 1) - 3 \n",
    "        rey_negro_columna = (4 - columna if columna < 4 else (columna % 4)) - 3\n",
    "        rincon = max(rey_negro_fila, rey_negro_columna)\n",
    "        return rincon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(ReyTorreRey,\"eval\",eval)\n",
    "roo = ReyTorreRey(tablero_inicial=3)\n",
    "s = roo.estado_inicial\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roo.eval(s, roo.a_jugar(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3:**\n",
    "\n",
    "Sobreviva el ataque de las blancas, las cuales juegan usando la función de evaluación anteriormente descrita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cutoff(self, board, d):\n",
    "    if self.es_terminal(board):\n",
    "        return True\n",
    "    elif d >= max_lim:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def H_minimax_alfa_beta(problema, board, d, alfa, beta):\n",
    "#    if d == 0:\n",
    "#        lista = []\n",
    "#        for a in problema.acciones(board):\n",
    "#            board_resultado = problema.resultado(board, a)\n",
    "#            v2, a2 = H_minimax_alfa_beta(problema, board_resultado, d+1, alfa, beta)\n",
    "#            lista.append((v2,a))\n",
    "#        print(lista)\n",
    "    jugador = problema.a_jugar(board)\n",
    "#    print(\"Jugador:\", jugador)\n",
    "    if problema.is_cutoff(board, d):\n",
    "#        print(\"feval:\", problema.eval(board, jugador))\n",
    "        return problema.eval(board, jugador) - 0.1*d, None\n",
    "    elif jugador == 'blancas':\n",
    "        v = -np.infty\n",
    "        for a in problema.acciones(board):\n",
    "            board_resultado = problema.resultado(board, a)\n",
    "            v2, a2 = H_minimax_alfa_beta(problema, board_resultado, d+1, alfa, beta)\n",
    "            if v2 > v:\n",
    "                v = v2\n",
    "                accion = a\n",
    "                alfa = max(alfa, v)\n",
    "            if v >= beta:\n",
    "                return v, accion\n",
    "        return v, accion\n",
    "    elif jugador == 'negras':\n",
    "        v = np.infty\n",
    "        for a in problema.acciones(board):\n",
    "            board_resultado = problema.resultado(board, a)\n",
    "            v2, a2 = H_minimax_alfa_beta(problema, board_resultado, d+1, alfa, beta)\n",
    "            if v2 < v:\n",
    "                v = v2\n",
    "                accion = a\n",
    "                beta = min(beta, v)\n",
    "            if v <= alfa:\n",
    "                return v, accion\n",
    "        return v, accion\n",
    "    else:\n",
    "        raise NameError(\"Oops!\")            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambientes import ReyTorreRey\n",
    "from IPython.display import clear_output, display, Image\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "###################\n",
    "# Búsqueda con profundidad limitada\n",
    "max_lim = 4\n",
    "setattr(ReyTorreRey,\"is_cutoff\",is_cutoff)\n",
    "setattr(ReyTorreRey,\"eval\",eval)\n",
    "roo = ReyTorreRey(tablero_inicial=3)\n",
    "s = roo.estado_inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduzca aquí su jugada y corra la celda\n",
    "a = 'Kd4'\n",
    "###################\n",
    "s = roo.jugada_manual(s, a)\n",
    "display(s)\n",
    "if not roo.es_terminal(s):\n",
    "    v, a = H_minimax_alfa_beta(roo, s, 0, -np.infty, np.infty)\n",
    "    s = roo.resultado(s, a)\n",
    "    sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    display(s)\n",
    "    if roo.es_terminal(s):\n",
    "        print('Juego terminado. ¡Ganan las blancas!')\n",
    "else:\n",
    "    jugador = roo.a_jugar(s)\n",
    "    if roo.utilidad(s, jugador)==0:\n",
    "        print('Juego terminado. ¡Tablas!')\n",
    "    else:\n",
    "        print('Juego terminado. ¡Ganan las negras!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La anterior función de evaluación no es muy buena. La siguiente función considerará una combinación lineal de los siguientes criterios:\n",
    "\n",
    "* Contar material, para no dejar que el rey negro tome la torre.\n",
    "* Rey en la orilla, para favorecer tableros en los que el rey negro esté acorralado.\n",
    "* Favorecer la oposición de los reyes. Para hacer mate, los reyes deben estar en oposición.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(self, board, jugador):\n",
    "    if self.es_terminal(board):\n",
    "#        print(\"Utilidad\", jugador, self.utilidad(board, jugador))\n",
    "        return self.utilidad(board, jugador)\n",
    "    else:\n",
    "        th1 = 2\n",
    "        th2 = 5\n",
    "        th3 = 1.6\n",
    "        # Contamos material\n",
    "        piezas = re.findall(r\"[\\w]+\", str(board))\n",
    "        dict_material = {'K':9, 'R':5, 'k':-9}\n",
    "        piezas = [dict_material[p] for p in piezas]\n",
    "        material = np.sum(piezas)\n",
    "        # Contamos rey negro en borde\n",
    "        fila_rey_negro, columna_rey_negro = self.casilla_pieza(board, 'k')\n",
    "        rey_negro_fila = (4 - fila_rey_negro if fila_rey_negro < 4 else (fila_rey_negro % 4) + 1) - 3 \n",
    "        rey_negro_columna = (4 - columna_rey_negro if columna_rey_negro < 4 else (columna_rey_negro % 4)) - 3\n",
    "        rincon = max(rey_negro_fila, rey_negro_columna)\n",
    "        # Contamos oposición\n",
    "        fila_rey_blanco, columna_rey_blanco = self.casilla_pieza(board, 'K')\n",
    "        oposicion = 1./(np.abs(fila_rey_blanco-fila_rey_negro) + np.abs(columna_rey_blanco-columna_rey_negro) + 1)\n",
    "        return th1*material + th2*rincon + th3*oposicion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4:**\n",
    "\n",
    "Ahora trate de sobrevivir el asalto de las blancas, las cuales usan esta función de evaluación repotenciada. La clave es tratar de mantener el rey negro en el centro. ¡Buena suerte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambientes import ReyTorreRey\n",
    "from IPython.display import clear_output, display, Image\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "###################\n",
    "# Búsqueda con profundidad limitada\n",
    "max_lim = 4\n",
    "setattr(ReyTorreRey,\"is_cutoff\",is_cutoff)\n",
    "setattr(ReyTorreRey,\"eval\",eval)\n",
    "roo = ReyTorreRey(tablero_inicial=1)\n",
    "s = roo.estado_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Introduzca aquí su jugada y corra la celda\n",
    "a = 'Kf7'\n",
    "###################\n",
    "s = roo.jugada_manual(s, a)\n",
    "display(s)\n",
    "if not roo.es_terminal(s):\n",
    "    v, a = H_minimax_alfa_beta(roo, s, 0, -np.infty, np.infty)\n",
    "    s = roo.resultado(s, a)\n",
    "    sleep(.5)\n",
    "    clear_output(wait=True)\n",
    "    display(s)\n",
    "    if roo.es_terminal(s):\n",
    "        print('Juego terminado. ¡Ganan las blancas!')\n",
    "else:\n",
    "    jugador = roo.a_jugar(s)\n",
    "    if roo.utilidad(s, jugador)==0:\n",
    "        print('Juego terminado. ¡Tablas!')\n",
    "    else:\n",
    "        print('Juego terminado. ¡Ganan las negras!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acciones = ['Kd4', 'Kd5', 'Kd4', 'Kc4', 'Kc3', 'Kc2', 'Kc3', 'Kc2', 'Kc1', 'Kb1', 'Kc1', 'Kb1', 'Kc1', 'Kb1']\n",
    "###################\n",
    "for a in acciones:\n",
    "    s = roo.jugada_manual(s, a)\n",
    "    display(s)\n",
    "    if not roo.es_terminal(s):\n",
    "        v, a = H_minimax_alfa_beta(roo, s, 0, -np.infty, np.infty)\n",
    "        s = roo.resultado(s, a)\n",
    "        sleep(.5)\n",
    "        clear_output(wait=True)\n",
    "        display(s)\n",
    "        if roo.es_terminal(s):\n",
    "            print('Juego terminado. ¡Ganan las blancas!')\n",
    "    else:\n",
    "        jugador = roo.a_jugar(s)\n",
    "        if roo.utilidad(s, jugador)==0:\n",
    "            print('Juego terminado. ¡Tablas!')\n",
    "        else:\n",
    "            print('Juego terminado. ¡Ganan las negras!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En este notebook usted aprendió\n",
    "\n",
    "* El algoritmo minimax para la toma de decisiones perfecta en un juego competitivo de 2 jugadores.\n",
    "* El algoritmo de poda alfa beta para eliminar porciones del árbol de confrontación que no influyen en la toma de decisiones.\n",
    "* Funciones de evaluación para aproximar qué tan buena es una posición para cada jugador."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
